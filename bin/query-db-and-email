#!/usr/bin/env node
'use strict';

const path = require('path');
const spawn = require('child_process').spawn;
const fs = require('fs-promise');
const neodoc = require('neodoc');
const nodemailer = require('nodemailer');
const request = require('request');
const stream = require('stream');
const formatCsv = require('../lib/formatters/csv');

const args = neodoc.run(`
Query our database for updated pages/versions and send an e-mail with details.

Usage: query-db-and-email [options]

Options:
  -h, --help              Print this lovely help message.
  --after HOURS           Only include versions from N hours ago. [default: 72]
  --before HOURS          Only include versions from before N hours ago.
  --db-url STRING         URL for web-monitoring-db instance [env: WEB_MONITORING_URL]
                          [default: https://api.monitoring.envirodatagov.org/]
  --ui-url STRING         URL for web-monitoring-ui instance [env: WEB_MONITORING_UI_URL]
                          [default: https://monitoring.envirodatagov.org/]
  --link-to-versionista   Create view/diff links to Versionista instead of Web-Monitoring
  --source-type TYPE      Restrict query to only versions with the given
                          \`source_type\` field (as in the DB API). Use \`ANY\`
                          for any source type. [default: versionista]
  --output DIRECTORY      Write output to this directory.
  --sender-email STRING   E-mail address to send from. [env: SEND_ARCHIVES_FROM]
  --sender-password PASS  Password for e-mail account to send from. [env: SEND_ARCHIVES_PASSWORD]
  --receiver-email STRING E-mail address to send results to [env: SEND_ARCHIVES_TO]
  --chunk-size NUMBER     Number of records to fetch at a time [default: 100]
  --chunk-delay SECONDS   Number of seconds to wait between chunks [default: 0]
  --debug                 Print debug messages
`);

process.on('unhandledRejection', (reason, p) => {
  console.error('Unhandled Rejection at:', p, 'reason:', reason);
  console.error(reason.stack);
});

const scriptsPath = __dirname;
const outputParent = path.resolve(args['--output']);
const dbUrl = (args['--db-url'])
  .split(',')
  .map(dbUrl => dbUrl.trim())
  .filter(dbUrl => !!dbUrl)
  .map(dbUrl => dbUrl.endsWith('/') ? dbUrl : (dbUrl + '/'))
  [0];
const dbCredentials = getCredentialsFromUrl(dbUrl);
const uiUrl = args['--ui-url'] + (args['--ui-url'].endsWith('/') ? '' : '/');
const linkToVersionista = args['--link-to-versionista'];

let sourceType = args['--source-type'];
if (sourceType === 'ANY') {
  sourceType = undefined;
}

const senderEmailName = 'EDGI Versionista Scraper'
const scrapeTime = new Date();
// Use ISO Zulu time without seconds in our time strings
const timeString = scrapeTime.toISOString().slice(0, 16) + 'Z';
const safeScrapeTime = timeString.replace(/:/g, '-');
const outputDirectory = path.join(outputParent, `versionista-${safeScrapeTime}`);


let startTime;
if (typeof args['--after'] === 'string' && args['--after'].includes('-')) {
  startTime = new Date(args['--after']);
  if (isNaN(startTime)) {
    throw new Error('--after should be and ISO 8601 date or a number of hours');
  }
}
else {
  const hours = Number(args['--after'])
  if (isNaN(hours)) {
    throw new Error('--after should be and ISO 8601 date or a number of hours');
  }
  startTime = new Date(Date.now() - hours * 60 * 60 * 1000);
}

let endTime = scrapeTime;
if (args['--before']) {
  if (typeof args['--before'] === 'string' && args['--before'].includes('-')) {
    endTime = new Date(args['--before']);
    if (isNaN(endTime)) {
      throw new Error('--before should be and ISO 8601 date or a number of hours');
    }
  }
  else {
    const hours = Number(args['--before'])
    if (isNaN(hours)) {
      throw new Error('--before should be and ISO 8601 date or a number of hours');
    }
    endTime = new Date(Date.now() - hours * 60 * 60 * 1000);
  }
}

const chunkDelay = Math.max(0, Number(args['--chunk-delay'])) * 1000;
const chunkSize = Number(args['--chunk-size']) || 100;

class SetMap extends Map {
  get (key) {
    let value = super.get(key);
    if (!value) {
      value = new Set();
      this.set(key, value);
    }
    return value;
  }

  add (key, item) {
    return this.get(key).add(item);
  }
}

let removeOutput = true;
fs.ensureDir(outputDirectory)
  .then(() => getSiteUpdates())
  .then(result => writeCsvsForSites(result.pagesBySite).then(() => result))
  .then(result => {
    return compressPath(outputDirectory).then(compressed => ({
      text: `Found ${result.siteCount} sites with updates
Found ${result.pageCount} pages with updates
Completed in ${result.queryDuration / 1000} seconds`,
      path: compressed
    }));
  })
  .catch(error => {
    // keep output for debugging if there was an error
    removeOutput = false;
    // write to console, but also send in e-mail
    console.error(error.stack || error)
    return error;
  })
  .then(sendResults)
  .catch(error => console.error(error.stack || error))
  .then(() => {
    if (removeOutput) {
      run(`rm`, ['-rf', path.join(outputParent, '*')], {shell: true});
    }
  });

function createViewUrl (page, toVersion, fromVersion, useVersionista) {
  if (useVersionista != null ? useVersionista : linkToVersionista) {
    const metadata = (toVersion || page.latest).source_metadata;
    let url = `https://versionista.com/${metadata.site_id}/${metadata.page_id}/`;
    if (fromVersion) {
      url = metadata.diff_with_first_url;
    }
    else if (toVersion) {
      url = metadata.diff_with_previous_url;
    }
    if (url && !url.endsWith('/')) {
      url = url + '/';
    }
    return url || '';
  }

  let url = `${uiUrl}page/${page.uuid}/`;
  if (toVersion) {
    url += `${fromVersion ? fromVersion.uuid : ''}..${toVersion.uuid}`;
  }

  return url;
}

function getSiteUpdates () {
  const pagesBySite = new SetMap();

  const timeframePages = getAllResults('api/v0/pages', {
    capture_time: `${startTime.toISOString()}..${endTime.toISOString()}`,
    source_type: sourceType,
    chunk_size: chunkSize
  })

  // Looking up all the versions in one shot is much more efficient than
  // looking up the versions for each page individually
  const timeframeVersions = getAllResults('api/v0/versions', {
    capture_time: `${startTime.toISOString()}..${endTime.toISOString()}`,
    source_type: sourceType,
    chunk_size: chunkSize,
    sort: 'capture_time:desc'
  })

  return Promise.all([timeframePages, timeframeVersions])
    .then(([pages, versions]) => {
      // Create a lookup tabel for pages
      const pagesById = new Map();
      pages.forEach(page => {
        pagesById.set(page.uuid, page)
        page.versions = [];
      });

      // Attach each version to its page
      versions.forEach(version => {
        const page = pagesById.get(version.page_uuid);
        if (!page) {
          console.error(`Warning: no matching page found for version ${version.uuid}`);
          return;
        }
        page.versions.push(version);
      });

      return pages;
    })
    // Add an `earliest` property to each page with its first version
    .then(pages => {
      if (linkToVersionista) return pages;

      return parallelMap(page => {
        return getResponse(`/api/v0/pages/${page.uuid}/versions`, {
          source_type: sourceType,
          sort: 'capture_time:asc',
          chunk_size: 1,
        })
          .then(body => {
            page.earliest = body.data[0];
            return page;
          });
      }, 10, pages);
    })
    // Group pages by site
    .then(pages => {
      pages.forEach(page => {
        page.site = page.tags
          .filter(tag => tag.name.startsWith('site:'))
          .map(tag => tag.name.replace(/^site:/, ''))
          .sort()[0] || 'no site';

        const latest = page.versions[0];
        page.latest = latest;
        if (!latest) {
          // Don't add a page to the list if it has no versions.
          console.warn(`Found page with no versions: ${page.uuid}`);
          return;
        }

        // Check whether there was also a non-error version that we should show
        if (isError(latest)) {
          pagesBySite.add('errors', page);

          for (let i = 1, len = page.versions.length; i < len; i++) {
            const version = page.versions[i];
            if (!isError(version)) {
              const nonErrorPage = Object.assign({}, page, {latest: version});
              pagesBySite.add(page.site, nonErrorPage);
              break;
            }
          }
        }
        else {
          pagesBySite.add(page.site, page);
        }
      });

      return {
        pagesBySite,
        pageCount: pages.length,
        siteCount: pagesBySite.size,
        queryDuration: Date.now() - scrapeTime.getTime()
      };
    });
}

function isError (version) {
  return version.source_metadata.errorCode
    || version.source_metadata.error_code;
}

function writeCsvsForSites (pagesBySite) {
  return Promise.all([...pagesBySite].map(([site, pages]) => {
    const csv = csvStringForPages([...pages]);
    const filename = `${site}_${safeScrapeTime}.csv`.replace(/[:/]/g, '_');
    return fs.writeFile(path.join(outputDirectory, filename), csv);
  }));
}

function csvStringForPages (pages) {
  const rows = pages
    .map(page => {
      const earliest = page.earliest || {uuid: '', metadata: {}};
      const version = page.latest || {uuid: '', metadata: {}};
      const metadata = version.source_metadata;
      return [
        '',
        version.uuid,
        // TODO: format
        timeString,
        page.maintainers.map(maintainer => maintainer.name).join(', '),
        page.site,
        page.title,
        page.url,
        // for now, the "page view" link is always to Versionista
        createViewUrl(page, null, null, true),
        createViewUrl(page, version),
        createViewUrl(page, version, earliest),
        formatCsv.formatDate(new Date(version.capture_time)),
        earliest.capture_time ? formatCsv.formatDate(new Date(earliest.capture_time)) : '----',
        metadata.diff_length,
        metadata.diff_hash,
        metadata.diff_text_length,
        metadata.diff_text_hash
      ];
    })

  return formatCsv.toCsvString([formatCsv.headers, ...(formatCsv.sortRows(rows))]);
}

function compressPath (targetPath) {
  const cwd = path.dirname(targetPath);
  const inFile = path.basename(targetPath);
  const outFile = `${inFile}.tar.gz`;

  return run('tar', ['-czf', outFile, inFile], {cwd})
    .then(process => {
      if (process.code !== 0) {
        throw new Error(`Failed to compress ${targetPath}: ${process.allIo}`);
      }
      return path.join(cwd, outFile);
    });
}

function sendResults (result) {
  return new Promise((resolve, reject) => {
    const friendlyDate = timeString.replace('T', ' ').replace('Z', ' (GMT)');
    const friendlyTime = friendlyDate.slice(11);

    const message = {
      from: `"${senderEmailName}" <${args['--sender-email']}>`,
      to: args['--receiver-email']
    };
    const sourceName = sourceType || 'all';
    const subjectDetails = `${sourceName} versions @ ${friendlyDate}`
    let signature = randomItem([
      '- Your friendly scraperbot',
      '- Your friendly scraperbot',
      '- Scrape-y McScrapesalot',
      '- Your faithful Scraperbot'
    ]);
    signature = `\n\n${signature}\n\n`;

    if (result instanceof Error) {
      message.subject = `[Experimental DB Query] Error scraping ${subjectDetails}`;

      const greeting = randomItem([
        'Uhoh,',
        'Troubles:',
        'Problems :(',
        '💩!'
      ]);

      message.text = `${greeting}\n\nThere was an error scraping the last ${args['--after']} hours of ${sourceName} versions out of our DB at ${friendlyTime}:\n\n${result.rawText || result.message}\n\n${result.stack}\n${signature}`;
    }
    else {
      message.subject = `[Experimental DB Query] Scraped ${subjectDetails}`;
      if (linkToVersionista) {
        message.subject += ' [with Versionista links]'
      }

      const greeting = randomItem([
        'Hi!',
        'Howdy!',
        'Good Morning!',
        'Mornin’!',
        'Hey,',
        'Hot off the server!',
        'Headed your way!'
      ]);

      const linkDestination = linkToVersionista ? 'Versionista' : 'the Web Monitoring UI';
      message.text = `${greeting}\n\nI scraped the last ${args['--after']} hours of ${sourceName} versions out of our DB at ${friendlyTime}.\nThe links in this data point to ${linkDestination}.\n\n${result.text}${signature}`;
      message.attachments = [{path: result.path}];
    }

    const transporter = nodemailer.createTransport({
      service: 'gmail',
      auth: {
        user: args['--sender-email'],
        pass: args['--sender-password']
      }
    });

    transporter.sendMail(message, (error, result) => {
      if (error) reject(error);
      else resolve(result);
    });
  });
}

function run (command, args, options = {}) {
  return new Promise((resolve, reject) => {
    let allIo = '';
    let stdout = '';
    let stderr = '';

    const child = spawn(command, args, Object.assign(options, {
      stdio: 'pipe'
    }))
      .on('error', reject)
      .on('close', code => {
        resolve({
          code,
          allIo,
          stdout,
          stderr
        });
      });

    child.stdout.on('data', data => {
      allIo += data;
      stdout += data;
      process.stdout.write(data);
    });

    child.stderr.on('data', data => {
      allIo += data;
      stderr += data;
      process.stderr.write(data);
    });
  });
}

function randomItem (items) {
  return items[Math.floor(Math.random() * items.length)]
}

const chunk_expression = /chunk=(\d+)/;

function getChunk (url) {
  return (url.match(chunk_expression) || [])[1] || 1;
}

function getCredentialsFromUrl (url) {
  const matchedCredentials = url.match(/^https?:\/\/([^/@]+):([^/@]+)@/);

  if (matchedCredentials) {
    return {
      username: decodeURIComponent(matchedCredentials[1]),
      password: decodeURIComponent(matchedCredentials[2])
    };
  }

  return null;
}

function getResponse (apiPath, qs) {
  const requestOptions = {qs};
  if (dbCredentials) {
    requestOptions.auth = dbCredentials;
  }

  return new Promise((resolve, reject) => {
    const url = /^http(s?):\/\//.test(apiPath) ? apiPath : `${dbUrl}${apiPath}`;
    request.get(url, requestOptions, function(error, response) {
      if (args['--debug']) {
        console.log(`Got ${url}`);
      }

      if (error) return reject(error);

      let body;
      try {
        body = JSON.parse(response.body);
      }
      catch (error) {
        throw new Error(
          `Could not parse response for ${url}\n\n${response.body}`);
      }

      if (response.statusCode !== 200) {
        return reject(body.errors[0]);
      };

      resolve(body);
    });
  });
}

function delayPromise (timeout) {
  return new Promise(done => timeout ? setTimeout(done, timeout) : done());
}

// Get all pages of a result set from the given API endpoint and query data
function getAllResults (apiPath, qs) {
  return getResponse(apiPath, qs)
    .then(body => {
      if (args['--debug']) {
        console.log(`Got ${getChunk(apiPath)} of ${getChunk(body.links.last)}`);
      }

      // Concatenate the next page onto the end of this one if there is one.
      if (body.links.next) {
        return delayPromise(chunkDelay || 0)
          .then(() => getAllResults(body.links.next))
          .then(nextPages => body.data.concat(nextPages));
      }

      return body.data;
    });
}

// Map an async transform function to each item of an array in parallel
function parallelMap (transform, parallelOperations, data) {
  const result = new Array(data.length);
  let index = 0;
  let inProgress = 0;

  return new Promise((resolve, reject) => {
    // If we have parallel work slots available, grab an item and work on it
    const transformNextItem = () => {
      if (inProgress >= parallelOperations) return;

      if (index >= data.length) {
        if (inProgress === 0) resolve(result);
        return;
      }

      inProgress++;
      const itemIndex = index++;
      Promise.resolve(data[itemIndex])
        .then(transform)
        .then(output => {
          result[itemIndex] = output;
          inProgress--;
          transformNextItem();
        })
        .catch(reject);
    };

    // Start working on as many items as possible
    while (inProgress < Math.min(data.length, parallelOperations)) {
      transformNextItem();
    }
  });
}
