#!/usr/bin/env node
'use strict';

const {compareMany, ascend, descend, getDeep} = require('../lib/tools');
const flatten = require('../lib/flatten');
const formatCsv = require('../lib/formatters/csv');
const fs = require('fs-promise');
const neodoc = require('neodoc');
const nodemailer = require('nodemailer');
const path = require('path');
const request = require('request');
const spawn = require('child_process').spawn;
const stream = require('stream');

const args = neodoc.run(`
Query our database for updated pages/versions and send an e-mail with details.

Usage: query-db-and-email [options]

Options:
  -h, --help              Print this lovely help message.
  --after HOURS           Only include versions from N hours ago. [default: 72]
  --before HOURS          Only include versions from before N hours ago.
  --db-url STRING         URL for web-monitoring-db instance [env: WEB_MONITORING_URL]
                          [default: https://api.monitoring.envirodatagov.org/]
  --ui-url STRING         URL for web-monitoring-ui instance [env: WEB_MONITORING_UI_URL]
                          [default: https://monitoring.envirodatagov.org/]
  --link-to-versionista   Create view/diff links to Versionista instead of Web-Monitoring
  --source-type TYPE      Restrict query to only versions with the given
                          \`source_type\` field (as in the DB API). Use \`ANY\`
                          for any source type. [default: versionista]
  --output DIRECTORY      Write output to this directory.
  --sender-email STRING   E-mail address to send from. [env: SEND_ARCHIVES_FROM]
  --sender-password PASS  Password for e-mail account to send from. [env: SEND_ARCHIVES_PASSWORD]
  --receiver-email STRING E-mail address to send results to [env: SEND_ARCHIVES_TO]
  --chunk-size NUMBER     Number of records to fetch at a time [default: 100]
  --chunk-delay SECONDS   Number of seconds to wait between chunks [default: 0]
  --debug                 Print debug messages
  --group-by TAG          Group resulting sheets by tag prefix. [default: site:]
`);

process.on('unhandledRejection', (reason, p) => {
  console.error('Unhandled Rejection at:', p, 'reason:', reason);
  console.error(reason.stack);
});

const scriptsPath = __dirname;
const outputParent = path.resolve(args['--output']);
const dbUrl = (args['--db-url'])
  .split(',')
  .map(dbUrl => dbUrl.trim())
  .filter(dbUrl => !!dbUrl)
  .map(dbUrl => dbUrl.endsWith('/') ? dbUrl : (dbUrl + '/'))
  [0];
const dbCredentials = getCredentialsFromUrl(dbUrl);
const uiUrl = args['--ui-url'] + (args['--ui-url'].endsWith('/') ? '' : '/');
const linkToVersionista = args['--link-to-versionista'];
const tagGroup = args['--group-by'];

let sourceType = args['--source-type'];
if (sourceType === 'ANY') {
  sourceType = undefined;
}

const senderEmailName = 'EDGI Versionista Scraper'
const scrapeTime = new Date();
// Use ISO Zulu time without seconds in our time strings
const timeString = scrapeTime.toISOString().slice(0, 16) + 'Z';
const safeScrapeTime = timeString.replace(/:/g, '-');
const outputDirectory = path.join(outputParent, `webmonitoring-${tagGroup.replace(/:/g, '')}s-${safeScrapeTime}`);


let startTime;
if (typeof args['--after'] === 'string' && args['--after'].includes('-')) {
  startTime = new Date(args['--after']);
  if (isNaN(startTime)) {
    throw new Error('--after should be and ISO 8601 date or a number of hours');
  }
}
else {
  const hours = Number(args['--after'])
  if (isNaN(hours)) {
    throw new Error('--after should be and ISO 8601 date or a number of hours');
  }
  startTime = new Date(Date.now() - hours * 60 * 60 * 1000);
}

let endTime = scrapeTime;
if (args['--before']) {
  if (typeof args['--before'] === 'string' && args['--before'].includes('-')) {
    endTime = new Date(args['--before']);
    if (isNaN(endTime)) {
      throw new Error('--before should be and ISO 8601 date or a number of hours');
    }
  }
  else {
    const hours = Number(args['--before'])
    if (isNaN(hours)) {
      throw new Error('--before should be and ISO 8601 date or a number of hours');
    }
    endTime = new Date(Date.now() - hours * 60 * 60 * 1000);
  }
}

const chunkDelay = Math.max(0, Number(args['--chunk-delay'])) * 1000;
const chunkSize = Number(args['--chunk-size']) || 100;

class SetMap extends Map {
  get (key) {
    let value = super.get(key);
    if (!value) {
      value = new Set();
      this.set(key, value);
    }
    return value;
  }

  add (key, item) {
    return this.get(key).add(item);
  }
}

let removeOutput = true;
fs.ensureDir(outputDirectory)
  .then(() => getGroupUpdates())
  .then(result => writeCsvsForGroups(result.pagesByGroup).then(() => result))
  .then(result => {
    return compressPath(outputDirectory).then(compressed => ({
      text: `Found ${result.groupCount} groups with updates
Found ${result.pageCount} pages with updates
Completed in ${result.queryDuration / 1000} seconds`,
      path: compressed
    }));
  })
  .catch(error => {
    // keep output for debugging if there was an error
    removeOutput = false;
    // write to console, but also send in e-mail
    console.error(error.stack || error)
    return error;
  })
  .then(sendResults)
  .catch(error => console.error(error.stack || error))
  .then(() => {
    if (removeOutput) {
      run(`rm`, ['-rf', path.join(outputParent, '*')], {shell: true});
    }
  });

function createViewUrl (page, toVersion, fromVersion, useVersionista) {
  if (useVersionista != null ? useVersionista : linkToVersionista) {
    const metadata = (toVersion || page.latest).source_metadata;
    let url = `https://versionista.com/${metadata.site_id}/${metadata.page_id}/`;
    if (fromVersion) {
      url = metadata.diff_with_first_url;
    }
    else if (toVersion) {
      url = metadata.diff_with_previous_url;
    }
    if (url && !url.endsWith('/')) {
      url = url + '/';
    }
    return url || '';
  }

  let url = `${uiUrl}page/${page.uuid}/`;
  if (toVersion) {
    url += `${fromVersion ? fromVersion.uuid : ''}..${toVersion.uuid}`;
  }

  return url;
}

function getGroupUpdates () {
  const pagesByGroup = new SetMap();

  const timeframePages = getAllResults('api/v0/pages', {
    capture_time: `${startTime.toISOString()}..${endTime.toISOString()}`,
    source_type: sourceType,
    chunk_size: chunkSize,
    include_earliest: true,
    active: true
  })
  .catch(error => {
    console.error(`Failed to get all pages: ${error}`);
    return Promise.reject(error);
  });

  // Looking up all the versions in one shot is much more efficient than
  // looking up the versions for each page individually
  const timeframeVersions = getAllResults('api/v0/versions', {
    capture_time: `${startTime.toISOString()}..${endTime.toISOString()}`,
    source_type: sourceType,
    chunk_size: chunkSize,
    sort: 'capture_time:desc',
    different: true,
    include_change_from_previous: true
  })
  .catch(error => {
    console.error(`Failed to get all versions: ${error}`);
    return Promise.reject(error);
  });

  return Promise.all([timeframePages, timeframeVersions])
    .then(([pages, versions]) => {
      // Create a lookup table for pages
      const pagesById = new Map();
      pages.forEach(page => {
        pagesById.set(page.uuid, page)
        page.versions = [];
      });

      // Attach each version to its page
      versions.forEach(version => {
        const page = pagesById.get(version.page_uuid);
        if (!page) {
          console.error(`Warning: no matching page found for version ${version.uuid}`);
          return;
        }
        page.versions.push(version);
      });

      return pages;
    })
    // Group pages based on tags
    .then(pages => {
      pages.forEach(page => {
        page.group = page.tags
          .filter(tag => tag.name.startsWith(tagGroup))
          .map(tag => tag.name.slice(tagGroup.length))
          .sort()[0] || 'no group';

        const latest = page.versions[0];
        page.latest = latest;
        if (!latest) {
          // Don't add a page to the list if it has no versions.
          console.warn(`Found page with no versions: ${page.uuid}`);
          return;
        }

        // Check whether there was also a non-error version that we should show
        if (isError(latest)) {
          pagesByGroup.add('errors', page);

          for (let i = 1, len = page.versions.length; i < len; i++) {
            const version = page.versions[i];
            if (!isError(version)) {
              const nonErrorPage = Object.assign({}, page, {latest: version});
              pagesByGroup.add(page.group, nonErrorPage);
              break;
            }
          }
        }
        else {
          pagesByGroup.add(page.group, page);
        }
      });

      return {
        pagesByGroup,
        pageCount: pages.length,
        groupCount: pagesByGroup.size,
        queryDuration: Date.now() - scrapeTime.getTime()
      };
    });
}

function isError (version) {
  return version.source_metadata.errorCode
    || version.source_metadata.error_code;
}

function writeCsvsForGroups (pagesByGroup) {
  return Promise.all([...pagesByGroup].map(([group, pages]) => {
    const csv = csvStringForPages([...pages]);
    const filename = `${group}_${safeScrapeTime}.csv`.replace(/[:/]/g, '_');
    return fs.writeFile(path.join(outputDirectory, filename), csv);
  }));
}

// Create an object representing an annotation, whether one was present or not
function compileAnnotation (version) {
  const meta = version.source_metadata || {};
  return Object.assign({
    source_diff_length: meta.diff_length || 0,
    source_diff_hash: meta.diff_hash || '?',
    text_diff_length: meta.text_diff_length || 0,
    text_diff_hash: meta.text_diff_hash || '?'
  }, getDeep(version, 'change_from_previous', 'current_annotation'));
}

function csvStringForPages (pages) {
  const blankVersion = {uuid: '', source_metadata: {}};
  const entries = pages
    .map(page => {
      const earliest = page.earliest || blankVersion;
      const version = page.latest || blankVersion;
      earliest.capture_time = parseDate(earliest.capture_time);
      version.capture_time = parseDate(version.capture_time);
      const annotation = compileAnnotation(version);
      return {page, earliest, version, annotation};
    });

  // Creates a CSV row from an entry object (above)
  const createRow = ({page, earliest, version, annotation}, index) => {
    return [
      index + 1,
      version.uuid,
      // TODO: format
      timeString,
      page.maintainers.map(maintainer => maintainer.name).join(', '),
      page.group,
      cleanString(page.title),
      page.url,
      // for now, the "page view" link is always to Versionista
      createViewUrl(page, null, null, true),
      createViewUrl(page, version),
      createViewUrl(page, version, earliest),
      formatCsv.formatDate(version.capture_time),
      formatCsv.formatDate(earliest.capture_time) || '----',
      annotation.source_diff_length,
      formatHash(annotation.source_diff_hash),
      annotation.text_diff_length,
      formatHash(annotation.text_diff_hash),
      annotation.priority
    ];
  };

  // Group rows by text hash and sort those groups by their maximum priority.
  // Basically we want something like a priority sort, but we want to make sure
  // identical hashes stay together.
  const textHashGroups = new Map();
  entries.forEach(entry => {
    let group = textHashGroups.get(entry.annotation.text_diff_hash);
    if (!group) {
      group = [];
      textHashGroups.set(entry.annotation.text_diff_hash, group);
    }
    group.push(entry);
    group.priority = Math.max(group.priority || 0, entry.annotation.priority || 0);
  });

  const sortedGroups = Array.from(textHashGroups.values())
    .sort(descend('priority'))
    .map(group => group.sort(compareMany(
      ascend(x => x.annotation.source_diff_hash),
      descend(x => x.annotation.priority),
      ascend(x => x.version.capture_time)
    )));
  const sortedRows = flatten(sortedGroups).map(createRow);

  const headers = [...formatCsv.headers, 'priority'];
  return formatCsv.toCsvString([headers, ...sortedRows]);
}

function parseDate (date) {
  return date && new Date(date);
}

function formatHash (hash) {
  const isEmpty =
    hash === 'e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855' ||  // empty string
    hash === '4f53cda18c2baa0c0354bb5f9a3ecbe5ed12ab4d8e11ba873c2f11161202b945';    // empty JSON array (`[]`)

  return isEmpty ? '[no change]' : hash;
}

function cleanString (text) {
  if (!text) return '';
  return text.trim().replace(/[\n\s]+/g, ' ');
}

function compressPath (targetPath) {
  const cwd = path.dirname(targetPath);
  const inFile = path.basename(targetPath);
  const outFile = `${inFile}.tar.gz`;

  return run('tar', ['-czf', outFile, inFile], {cwd})
    .then(process => {
      if (process.code !== 0) {
        throw new Error(`Failed to compress ${targetPath}: ${process.allIo}`);
      }
      return path.join(cwd, outFile);
    });
}

function sendResults (result) {
  return new Promise((resolve, reject) => {
    const friendlyDate = timeString.replace('T', ' ').replace('Z', ' (GMT)');
    const friendlyTime = friendlyDate.slice(11);

    const message = {
      from: `"${senderEmailName}" <${args['--sender-email']}>`,
      to: args['--receiver-email']
    };
    const sourceName = sourceType || 'all';
    const subjectDetails = `${sourceName} versions @ ${friendlyDate} (grouped by ${tagGroup})`
    let signature = randomItem([
      '- Your friendly scraperbot',
      '- Your friendly scraperbot',
      '- Scrape-y McScrapesalot',
      '- Your faithful Scraperbot'
    ]);
    signature = `\n\n${signature}\n\n`;

    if (result instanceof Error) {
      message.subject = `[Task Sheets] Error scraping ${subjectDetails}`;

      const greeting = randomItem([
        'Uhoh,',
        'Troubles:',
        'Problems :(',
        '💩!'
      ]);

      message.text = `${greeting}\n\nThere was an error scraping the last ${args['--after']} hours of ${sourceName} versions out of our DB at ${friendlyTime}:\n\n${result.rawText || result.message}\n\n${result.stack}\n${signature}`;
    }
    else {
      message.subject = `[Task Sheets] Scraped ${subjectDetails}`;
      if (linkToVersionista) {
        message.subject += ' [with Versionista links]'
      }

      const greeting = randomItem([
        'Hi!',
        'Howdy!',
        'Good Morning!',
        'Mornin’!',
        'Hey,',
        'Hot off the server!',
        'Headed your way!'
      ]);

      const linkDestination = linkToVersionista ? 'Versionista' : 'the Web Monitoring UI';
      message.text = `${greeting}\n\nI scraped the last ${args['--after']} hours of ${sourceName} versions out of our DB at ${friendlyTime}.\nThe links in this data point to ${linkDestination}.\nThe CSVs are grouped by the “${tagGroup}” tag.\n\n${result.text}${signature}`;
      message.attachments = [{path: result.path}];
    }

    const transporter = nodemailer.createTransport({
      service: 'gmail',
      auth: {
        user: args['--sender-email'],
        pass: args['--sender-password']
      }
    });

    transporter.sendMail(message, (error, result) => {
      if (error) reject(error);
      else resolve(result);
    });
  });
}

function run (command, args, options = {}) {
  return new Promise((resolve, reject) => {
    let allIo = '';
    let stdout = '';
    let stderr = '';

    const child = spawn(command, args, Object.assign(options, {
      stdio: 'pipe'
    }))
      .on('error', reject)
      .on('close', code => {
        resolve({
          code,
          allIo,
          stdout,
          stderr
        });
      });

    child.stdout.on('data', data => {
      allIo += data;
      stdout += data;
      process.stdout.write(data);
    });

    child.stderr.on('data', data => {
      allIo += data;
      stderr += data;
      process.stderr.write(data);
    });
  });
}

function randomItem (items) {
  return items[Math.floor(Math.random() * items.length)]
}

const chunk_expression = /chunk=(\d+)/;

function getChunk (url) {
  return (url.match(chunk_expression) || [])[1] || 1;
}

function getCredentialsFromUrl (url) {
  const matchedCredentials = url.match(/^https?:\/\/([^/@]+):([^/@]+)@/);

  if (matchedCredentials) {
    return {
      username: decodeURIComponent(matchedCredentials[1]),
      password: decodeURIComponent(matchedCredentials[2])
    };
  }

  return null;
}

function getResponse (apiPath, qs) {
  const requestOptions = {qs};
  if (dbCredentials) {
    requestOptions.auth = dbCredentials;
  }

  return new Promise((resolve, reject) => {
    const url = /^http(s?):\/\//.test(apiPath) ? apiPath : `${dbUrl}${apiPath}`;
    getWithRetries(url, requestOptions, 3, function (error, response) {
      if (args['--debug']) {
        console.log(`Got ${url}`);
      }

      if (error) return reject(error);

      let body;
      try {
        body = JSON.parse(response.body);
      }
      catch (error) {
        throw new Error(
          `Could not parse response for ${url}\n\n${response.body}`);
      }

      if (response.statusCode !== 200) {
        return reject(body.errors[0]);
      };

      resolve(body);
    });
  });
}

// TODO: clean this up?
function getWithRetries(url, options, retries, callback) {
  if (typeof retries === 'function') {
    [callback, retries] = [retries, 2];
  }
  let retryCount = 0;

  function handleResult (error, response) {
    if ((error || response.status >= 500) && retryCount <= retries) {
      const delay = retryCount > 0 ? 1000 * Math.pow(2, retryCount - 1) : 0
      setTimeout(() => {
        retryCount++;
        request.get(url, options, handleResult);
      }, delay);
    }
    else {
      callback(error, response);
    }
  }

  request.get(url, options, handleResult);
}

function delayPromise (timeout) {
  return new Promise(done => timeout ? setTimeout(done, timeout) : done());
}

// Get all pages of a result set from the given API endpoint and query data
function getAllResults (apiPath, qs) {
  return getResponse(apiPath, qs)
    .then(body => {
      if (args['--debug']) {
        console.log(`Got ${getChunk(apiPath)} of ${getChunk(body.links.last)}`);
      }

      // Concatenate the next page onto the end of this one if there is one.
      if (body.links.next) {
        return delayPromise(chunkDelay || 0)
          .then(() => getAllResults(body.links.next))
          .then(nextPages => body.data.concat(nextPages));
      }

      return body.data;
    });
}
