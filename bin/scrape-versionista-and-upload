#!/usr/bin/env node
'use strict';

const sentryErrors = require('../lib/sentry-errors').setup();

const path = require('path');
const spawn = require('child_process').spawn;
const mkdirp = require('mkdirp');
const neodoc = require('neodoc');

const args = neodoc.run(`
Runs scrape-versionista, uploads the resulting files to Amazon S3 and Google
Cloud Storage, and finally imports them into an instance of web-monitoring-db
(https://github.com/edgi-govdata-archiving/web-monitoring-db/).

Usage: scrape-versionista-and-upload [options] --output DIRECTORY

Options:
  -h, --help                Print this lovely help message.
  --after HOURS             Only include versions from N hours ago. [default: 1]
  --before HOURS            Only include versions before N hours ago. [default: 0]
  --output DIRECTORY        Write output to this directory.
  --email STRING            Versionista account e-mail address [env: VERSIONISTA_EMAIL]
  --password STRING         Versionista account password [env: VERSIONISTA_PASSWORD]
  --account-name NAME       Name to use for Versionista account in output. [env: VERSIONISTA_NAME]
  --s3-key KEY              S3 access key [env: AWS_S3_KEY]
  --s3-secret SECRET        S3 secret key [env: AWS_S3_SECRET]
  --s3-bucket NAME          S3 bucket to upload to [env: AWS_S3_BUCKET]
  --google-project ID       Google Cloud project ID [env: GOOGLE_PROJECT_ID]
  --google-key PATH         Google Cloud key file [env: GOOGLE_STORAGE_KEY_FILE]
  --google-bucket NAME      Google Could bucket to upload to [env: GOOGLE_BUCKET]
  --throughput NUM          Number of simultaneous uploads to allow
  --db-email STRING         Login e-mail for web-monitoring-db [env: WEB_MONITORING_EMAIL]
  --db-password STRING      Password for web-monitoring-db [env: WEB_MONITORING_PASSWORD]
  --db-url STRING           URL for web-monitoring-db instance [env: WEB_MONITORING_URL]
  --scrape-parallel NUM     Number of parallel connections to Versionista allowed.
  --scrape-pause-every NUM  Pause briefly after this many requests to Versionista.
  --scrape-pause-time MS    Milliseconds to pause for (see --scrape-pause-every)
  --scrape-rate NUMBER      Maximum number of requests per minute
`);

const scriptsPath = __dirname;
const outputDirectory = args['--output'];

const scrapeTime = new Date();
// Use ISO Zulu time without seconds in our time strings
const timeString = scrapeTime.toISOString().slice(0, 16) + 'Z';
const throughput = args['--throughput'] ? (args['--throughput'] / 2) : 0;

const dbUrls = (args['--db-url'] || '')
  .split(',')
  .map(dbUrl => dbUrl.trim())
  .filter(dbUrl => !!dbUrl);

archiveAndUpload(args['--email'], args['--password'], error => {
  if (error) {
    console.error(error);
    sentryErrors.captureMessage(`scrape-versionista-and-upload: ${error}`)
      .then(() => process.exit(1));
  }
  else {
    console.error('Archive and upload complete!');
  }
});


function archiveAndUpload (email, password, callback) {
  const account = args['--account-name'] || email.match(/^(.+)@/)[1];
  const mainDirectory = path.join(outputDirectory, account);

  mkdirp(mainDirectory, error => {
    if (error) {
      return callback(error);
    }

    const timingOptions = ['parallel', 'pause-every', 'pause-time', 'rate']
      .reduce((result, name) => {
        const value = args[`--scrape-${name}`];
        if (value) {
          result.push(`--${name}`, value);
        }
        return result;
      }, []);

    const scraper = spawn(
      path.join(scriptsPath, 'scrape-versionista'),
      [
        '--email', email,
        '--password', password,
        '--account-name', account,
        '--after', args['--after'],
        '--before', args['--before'],
        '--format', 'json-stream',
        '--output', path.join(mainDirectory, `metadata-${timeString}.json`),
        '--errors', path.join(mainDirectory, `errors-${timeString}.log`),
        '--relative-paths', path.join(outputDirectory),
        '--save-content',
        '--save-diffs'
      ].concat(timingOptions),
      {
        stdio: 'inherit'
      });

    scraper.on('close', code => {
      if (code !== 0) {
        return callback(new Error(`Failed to scrape account ${email}`))
      }

      upload(account, callback);
    });
  });
}

function upload (account, callback) {
  const uploadDirectory = path.join(outputDirectory, account);
  let remaining = 2;
  let errors = [];
  function complete (error) {
    remaining--;
    if (error) {
      errors.push(error);
    }

    if (!remaining) {
      const message = errors.length ? new Error(errors.join('\n')) : null;
      callback(message);
    }
  }

  const s3 = spawn(
    path.join(scriptsPath, 'upload-to-s3'),
    [
      '--key', args['--s3-key'],
      '--secret', args['--s3-secret'],
      '--prefix', `${account}/`,
      '--throughput', throughput || 50,
      args['--s3-bucket'],
      uploadDirectory
    ],
    {
      stdio: 'inherit'
    });

  const google = spawn(
    path.join(scriptsPath, 'upload-to-google'),
    [
      '--keyfile', args['--google-key'],
      '--project', args['--google-project'],
      '--prefix', `${account}/`,
      // NOTE: Google is a little slow and can't take high throughput :(
      '--throughput', throughput || 10,
      args['--google-bucket'],
      uploadDirectory
    ],
    {
      stdio: 'inherit'
    });

  s3.on('close', code => {
    if (code == 0) {
      console.error('Successfully uploaded to S3');
      importToDbs(account, complete);
    }
    else {
      complete('Failed to upload to S3');
    }
  });
  google.on('close', code => {
    if (code == 0) {
      console.error('Successfully uploaded to Google Cloud');
    }
    complete(code ? 'Failed to upload to Google Cloud.' : null);
  });
}

function importToDbs (account, callback) {
  let remaining = dbUrls.length;

  if (remaining === 0) {
    console.error('Nothing imported to web-monitoring-db (not configured)');
    return callback();
  }

  for (let dbUrl of dbUrls) {
    importToDb(dbUrl, account, error => {
      remaining--;
      if (remaining === 0 || error) {
        remaining = 0;
        callback(error);
      }
    });
  }
}

function importToDb (dbUrl, account, callback) {
  const metadataPath = path.join(
    outputDirectory,
    account,
    `metadata-${timeString}.json`);

  const importProcess = spawn(
    path.join(scriptsPath, 'import-to-db'),
    [
      '--email', args['--db-email'],
      '--password', args['--db-password'],
      '--host', dbUrl,
      metadataPath
    ],
    {
      stdio: 'inherit'
    });

  importProcess.on('close', code => {
    const loggableUrl = urlWithoutAuth(dbUrl);
    if (code == 0) {
      console.error(`Successfully imported into ${loggableUrl}`);
    }
    callback(code ? `Failed to import to DB ${loggableUrl}.` : null);
  });
}

function urlWithoutAuth (url) {
  return url.replace(/^(\w+:\/\/)([^\/]+@)/, '$1');
}
